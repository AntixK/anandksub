<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/_css/ak.css><link rel=icon href=/assets/icons/favicon.ico type=image/x-icon><title></title></head><body><header><h1><a href=/ >Anand K Subramanian</a><br><br></h1></header><nav class=navbar><span><a href=/blog/ >Blog</a><span class=vl></span></span><span><a href=/notes/ >Notes</a> <span class=vl></span></span><span><a href=/art/ >Art</a> <span class=vl></span></span><span><a href=/CV>CV</a> <span class=vl></span></span><span><a href=/tags>Tags</a></nav><main><br><br><h1>Posts about #math</h1><table><colgroup><col span=1 style=width:30%;></colgroup><tr><td><p class=date>16 Mar 2025<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/depth_point_warp>Perspective Warping Based on Depth-Maps</a></h3><p class=tags>Warping images based on depth and camera viewpoints for novel view synthesis</p><p class=tags><span class=pound>#</span>computer-vision <span class=pound>#</span>geometric-projection <span class=pound>#</span>image-processing <span class=pound>#</span>math <span class=pound>#</span>torch <span class=pound>#</span>code <span class=pound>#</span>camera </p></td></tr><tr><td><p class=date>14 Jan 2025<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/FNO_trick>About that Trick in Fourier Neural Operators</a></h3><p class=tags>Notes on some implementation details of Fourier Neural Operators.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>fourier <span class=pound>#</span>ml <span class=pound>#</span>code <span class=pound>#</span>torch </p></td></tr><tr><td><p class=date>05 Dec 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Perspective_Equirect>Converting Between Perspective and Equirectangular Projections</a></h3><p class=tags>Inverting the common procedure of converting equirectangular images to perspective images.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>geometric-projection <span class=pound>#</span>image-processing <span class=pound>#</span>computer-vision <span class=pound>#</span>code </p></td></tr><tr><td><p class=date>25 Nov 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Tufted_Laplacian>Extending Discrete Laplacian to Non-Manifold Structures</a></h3><p class=tags>A simple way to construct the discrete Laplacian operator for messy point clouds and non-manifold meshes.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>geometry <span class=pound>#</span>point-clouds <span class=pound>#</span>graph <span class=pound>#</span>mesh </p></td></tr><tr><td><p class=date>26 Sep 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Reg_To_Cls>Classification to Regression and Back</a></h3><p class=tags>A trick to convert classification labels to regression targets and back.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code </p></td></tr><tr><td><p class=date>02 Aug 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Autodiff_Combinatorial_Solver>Differentiating Straight-Through Combinatorial Solvers</a></h3><p class=tags>Naive zeroth-order gradient estimation for backpropagating through combinatorial solvers.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code <span class=pound>#</span>gradient <span class=pound>#</span>deep-learning <span class=pound>#</span>combinatorial-solver </p></td></tr><tr><td><p class=date>18 Mar 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/l1_as_l2>L1 Regularization in terms of L2</a></h3><p class=tags>A short note on over-parameterizing the L1 regularizer to make it differentiable</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml </p></td></tr><tr><td><p class=date>12 Jan 2024<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/GMM_Mahalanobis>Extending Mahalanobis Distance to Gaussian Mixtures</a></h3><p class=tags>A simple generalization of Mahalanobis distance to Gaussian Mixture Models (GMMs).</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code </p></td></tr><tr><td><p class=date>04 Aug 2023<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Mathematics_of_Changing_Ones_Mind>Mathematics of Changing One's Mind</a></h3><p class=tags>A guide to updating probabilistic beliefs using Jeffrey's rule and Pearl's method.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>probability </p></td></tr><tr><td><p class=date>29 Aug 2022<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/improving_ransac>Improving the RANSAC Algorithm</a></h3><p class=tags>Discussion about the MAGSAC algorithm, addressing a crucial hyperparameter selection issue for the RANSAC algorithm.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code <span class=pound>#</span>jax </p></td></tr><tr><td><p class=date>18 Jul 2022<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/A_Detour_to_the_Imaginary>A Detour to the Imaginary has its Benefits</a></h3><p class=tags>Two examples of using complex numbers for real-function optimization.</p><p class=tags><span class=pound>#</span>numerics <span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>code <span class=pound>#</span>python </p></td></tr><tr><td><p class=date>29 Jun 2022<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Back_gradient_trick>The Back-Gradient Trick</a></h3><p class=tags>Stochastic Gradient Descent can be (kind of) reversed and can be used to compute gradients with respect to its hyperparameters.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>graph <span class=pound>#</span>code <span class=pound>#</span>jax <span class=pound>#</span>deep-learning </p></td></tr><tr><td><p class=date>31 May 2022<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/parallel_kalman>Parallelizing Kalman Filters</a></h3><p class=tags>The associative property of Kalman (Bayesian) filters can yield a parallel algorithm in O(log N). </p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>parallel <span class=pound>#</span>code <span class=pound>#</span>jax </p></td></tr><tr><td><p class=date>23 Sep 2021<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/fast-sample-covariance>Fast Sample-Covariance Computation for Multidimensional Arrays</a></h3><p class=tags>A quick discussion and a vectorized Python implementation for the computation of sample covariance matrices for multi-dimensional arrays.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code </p></td></tr><tr><td><p class=date>30 Aug 2021<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/asymmetric-numeral-systems>Asymmetric Numeral Systems</a></h3><p class=tags>A tutorial on the lossless Asymmetric Numeral Systems (ANS) coding commonly used in image compression. </p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>information-theory <span class=pound>#</span>code </p></td></tr><tr><td><p class=date>14 Aug 2021<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/A_Godelian_Argument_for_the_Superiority_of_the_Human_Mind>A Gödelian Argument for the Superiority of the Human Mind</a></h3><p class=tags>A discussion of an argument that no Turing Machine can adequately mimic human cognitive abilities, following Gödel's theorems.</p><p class=tags><span class=pound>#</span>philosophy <span class=pound>#</span>ai <span class=pound>#</span>math </p></td></tr><tr><td><p class=date>28 Jul 2021<p></td><td><h3 style=margin-top:0.5rem;><a href=/notes/godels-proof>Gödel's Proof - Ernest Nagel & James Newman</a></h3><p class=tags>Notes on the book 'Gödel's Proof' by Ernest Nagel and James Newman</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>philosophy <span class=pound>#</span>self-referential </p></td></tr><tr><td><p class=date>27 Jun 2021<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/A_Beautiful_Way_to_Characterize_Directed_Acyclic_Graphs>A Beautiful Way to Characterize Directed Acyclic Graphs</a></h3><p class=tags>An interesting connection between the number of cycles in a digraph and its power adjacency matrix leads to a beautiful formulation for DAG constrains.</p><p class=tags><span class=pound>#</span>graph <span class=pound>#</span>math <span class=pound>#</span>ml </p></td></tr><tr><td><p class=date>18 Jun 2020<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/clever-trick-reparam>A Cleverer Trick on top of the Reparametrization Trick</a></h3><p class=tags>Implicit differentiation can lead to an efficient computation of the gradient of reparametrized samples.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>deep-learning </p></td></tr><tr><td><p class=date>08 Jun 2020<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Trick_gradient_expectation>A Trick for Computing the gradient of Expectation</a></h3><p class=tags>A trick for interchanging the gradient and Expectation of a function under the Gaussian distribution.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>deep-learning </p></td></tr><tr><td><p class=date>18 Aug 2019<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Euclidean_GD_to_Natural_GD>Segue from Euclidean Gradient Descent to Natural Gradient Descent</a></h3><p class=tags>A slight change in SGD formulation, in terms of maximization of local approximation, leads to an interesting general connection to NGD via mirror descent.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>natural-gradient <span class=pound>#</span>deep-learning </p></td></tr><tr><td><p class=date>08 Aug 2019<p></td><td><h3 style=margin-top:0.5rem;><a href=/blog/Natural_gradient_exp_family>A Fascinating connection between Natural Gradients and the Exponential Family</a></h3><p class=tags>The Exponential family provides an elegant and easy method to compute Natural Gradients and thus can be used for Variational Inference.</p><p class=tags><span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>natural-gradient <span class=pound>#</span>deep-learning </p></td></tr></table></main><div style="font-size: 1.2rem; font-family: 'Overpass'; color: var(--c-4); text-align: center; margin-top: 6em; border-width: 75%; border-top: 1px solid var(--c-4); padding-top: 2em; margin-bottom: 4em;"> &copy; 2025 Anand K Subramanian <span class=vl></span><a href=/license>License</a> <span class=vl></span><a href=/design>Design</a> <span class=vl></span> Built with Kutti <span style="color: #e25555; font-size: 24px;">&hearts;</span></div></body></html>