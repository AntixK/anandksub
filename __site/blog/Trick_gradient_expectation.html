<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        throwOnError: false,
        // output: "html",
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true }, // block
          { left: '$', right: '$', display: false }, // Inline
          { left: "\begin{align}", right: "\end{align}", display: true }, // block
          { left: "\begin{aligned}", right: "\end{aligned}", display: true }, // block
        ],
        ignoredTags: [
          "pre",
          "code",
          "script",
          "style",
        ],
        macros: {
          "\\A": "\\mathbb{A}",
          "\\B": "\\mathbb{B}",
          "\\C": "\\mathbb{C}",
          "\\D": "\\mathbb{D}",
          "\\E": "\\mathbb{E}",
          "\\F": "\\mathbb{F}",
          "\\G": "\\mathbb{G}",
          "\\H": "\\mathbb{H}",
          "\\I": "\\mathbb{I}",
          "\\J": "\\mathbb{J}",
          "\\K": "\\mathbb{K}",
          "\\L": "\\mathbb{L}",
          "\\M": "\\mathbb{M}",
          "\\N": "\\mathbb{N}",
          "\\O": "\\mathbb{O}",
          "\\P": "\\mathbb{P}",
          "\\Q": "\\mathbb{Q}",
          "\\R": "\\mathbb{R}",
          "\\S": "\\mathbb{S}",
          "\\T": "\\mathbb{T}",
          "\\U": "\\mathbb{U}",
          "\\V": "\\mathbb{V}",
          "\\W": "\\mathbb{W}",
          "\\X": "\\mathbb{X}",
          "\\Y": "\\mathbb{Y}",
          "\\Z": "\\mathbb{Z}",

          "\\fA": "\\mathcal{A}",
          "\\fB": "\\mathcal{B}",
          "\\fC": "\\mathcal{C}",
          "\\fD": "\\mathcal{D}",
          "\\fE": "\\mathcal{E}",
          "\\fF": "\\mathcal{F}",
          "\\fG": "\\mathcal{G}",
          "\\fH": "\\mathcal{H}",
          "\\fI": "\\mathcal{I}",
          "\\fJ": "\\mathcal{J}",
          "\\fK": "\\mathcal{K}",
          "\\fL": "\\mathcal{L}",
          "\\fM": "\\mathcal{M}",
          "\\fN": "\\mathcal{N}",
          "\\fO": "\\mathcal{O}",
          "\\fP": "\\mathcal{P}",
          "\\fQ": "\\mathcal{Q}",
          "\\fR": "\\mathcal{R}",
          "\\fS": "\\mathcal{S}",
          "\\fT": "\\mathcal{T}",
          "\\fU": "\\mathcal{U}",
          "\\fV": "\\mathcal{V}",
          "\\fW": "\\mathcal{W}",
          "\\fX": "\\mathcal{X}",
          "\\fY": "\\mathcal{Y}",
          "\\fZ": "\\mathcal{Z}",

          "\\x": "\\times",
          "\\vA": "\\mathbf{A}",
          "\\vB": "\\mathbf{B}",
          "\\vC": "\\mathbf{C}",
          "\\vD": "\\mathbf{D}",
          "\\vE": "\\mathbf{E}",
          "\\vF": "\\mathbf{F}",
          "\\vG": "\\mathbf{G}",
          "\\vH": "\\mathbf{H}",
          "\\vI": "\\mathbf{I}",
          "\\vJ": "\\mathbf{J}",
          "\\vK": "\\mathbf{K}",
          "\\vL": "\\mathbf{L}",
          "\\vM": "\\mathbf{M}",
          "\\vN": "\\mathbf{N}",
          "\\vO": "\\mathbf{O}",
          "\\vP": "\\mathbf{P}",
          "\\vQ": "\\mathbf{Q}",
          "\\vR": "\\mathbf{R}",
          "\\vS": "\\mathbf{S}",
          "\\vT": "\\mathbf{T}",
          "\\vU": "\\mathbf{U}",
          "\\vV": "\\mathbf{V}",
          "\\vW": "\\mathbf{W}",
          "\\vX": "\\mathbf{X}",
          "\\vY": "\\mathbf{Y}",
          "\\vZ": "\\mathbf{Z}",

          "\\va": "\\mathbf{a}",
          "\\vb": "\\mathbf{b}",
          "\\vc": "\\mathbf{c}",
          "\\vd": "\\mathbf{d}",
          "\\ve": "\\mathbf{e}",
          "\\vf": "\\mathbf{f}",
          "\\vg": "\\mathbf{g}",
          "\\vh": "\\mathbf{h}",
          "\\vi": "\\mathbf{i}",
          "\\vj": "\\mathbf{j}",
          "\\vk": "\\mathbf{k}",
          "\\vl": "\\mathbf{l}",
          "\\vl": "\\mathbf{l}",
          "\\vm": "\\mathbf{m}",
          "\\vn": "\\mathbf{n}",
          "\\vo": "\\mathbf{o}",
          "\\vp": "\\mathbf{p}",
          "\\vq": "\\mathbf{q}",
          "\\vr": "\\mathbf{r}",
          "\\vs": "\\mathbf{s}",
          "\\vt": "\\mathbf{t}",
          "\\vu": "\\mathbf{u}",
          "\\vv": "\\mathbf{v}",
          "\\vw": "\\mathbf{w}",
          "\\vx": "\\mathbf{x}",
          "\\vy": "\\mathbf{y}",
          "\\vz": "\\mathbf{z}",

          "\\vmu": "\\boldsymbol \\mu",
          "\\vnu": "\\boldsymbol \\nu",
          "\\veps": "\\boldsymbol \\epsilon",
          "\\vtheta": "\\boldsymbol \\theta",
          "\\vomega": "\\boldsymbol \\omega",
          "\\vlambda": "\\boldsymbol \\lambda",
          "\\vzero": "\\mathbf{0}",
          "\\vone": "\\mathbf{1}",
          "\argmin": "\mathop{\mathrm{argmin}} \;",
          "\argmax": "\mathop{\mathrm{argmax}} \;",
          "\\KL": "\\mathop{D_{\\mathrm{KL}}} \\;",

        },
        globalGroup: true,
      });
    });
  </script><link rel=stylesheet href=/_libs/katex/katex.min.css><script defer src=/_libs/algorithm/algotype.js crossorigin=anonymous></script><script defer src=/_libs/katex/katex.min.js crossorigin=anonymous>
  </script></script><script defer src=/_libs/katex/auto-render.min.js crossorigin=anonymous onload=renderMathInElement(document.body);></script><link rel=stylesheet href=/_css/ak.css><link rel=icon href=/assets/icons/favicon.ico type=image/x-icon><title>A Trick for Computing the gradient of Expectation</title></head><body><header><h1><a href=/ >Anand K Subramanian</a><br><br></h1></header><nav class=navbar><span><a href=/blog/ >Blog</a><span class=vl></span></span><span><a href=/notes/ >Notes</a> <span class=vl></span></span><span><a href=/art/ >Art</a> <span class=vl></span></span><span><a href=/CV>CV</a> <span class=vl></span></span><span><a href=/tags>Tags</a></nav><p class=tags><img class=icon-image src=/assets/icons/bookmark.svg alt=clock-icon>&ensp;<span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>gradient <span class=pound>#</span>deep-learning </p><p class=tags><img class=icon-image src=/assets/icons/calendar.svg alt=clock-icon>&ensp;08 June 2020 </p><p class=tags><img class=icon-image src=/assets/icons/timer.svg alt=clock-icon>&ensp;3 mins </p><h1>A Trick for Computing the gradient of Expectation</h1><p class=tldr>A trick for interchanging the gradient and Expectation of a function under the Gaussian distribution.</p><main><p>  I recently came across this elegant trick for computing the gradient of any function written as an expectation with respect to a Multivariate Gaussian distribution, when I was reading about Natural Gradients.</p><p>Consider any function $f$ whose expectation is written as a $\mathbb{E}_q[f(\mathbf{x})]$, where $q = N(\mu, \Sigma)$ is a multivariate Gaussian distribution. Firstly, why this peculiar form and when do I need it? The above form is widely used in Machine Learning - especially in variational methods where $f(x)$ is usually the log-likelihood of your model and the standard method to maximize it is by pushing its lower bound(ELBO) using the another distribution $q$ (usually Gaussian).</p><p>The core idea behind this trick is to write the expectation in terms of the <em>characteristic function</em> $G(\mathbf{k})$ of the Gaussian $q$ as</p><p>$$ \begin{aligned} \mathbb{E}_q[f(\mathbf{x})] &amp;= \frac{1}{(2\pi)^n}\int G(\mathbf{k}) \int e^{-i\mathbf{k}^T\mathbf{y}}f(\mathbf{y})d\mathbf{y} d\mathbf{k} \\ &amp;= \frac{1}{(2\pi)^n}\int \int G(\mathbf{k})e^{-i\mathbf{k}^T\mathbf{y}}f(\mathbf{y})d\mathbf{y} d\mathbf{k}\\ &amp;= \frac{1}{(2\pi)^n}\int \int e^{-0.5\mathbf{k}^T\Sigma\mathbf{k}+i\mathbf{k}^T\mu}e^{-i\mathbf{k}^T\mathbf{y}}f(\mathbf{y})d\mathbf{y} d\mathbf{k} \end{aligned} $$</p><p>Note that the above formula is simply the area under the curve defined by the product of $G(\mathbf{k})$ (Fourier Transform of the Gaussian density function) and the Fourier Transform of $f(\mathbf{x})$. In the above equation, observe that taking the gradient with respect to $\mu$ is equivalent to taking the gradient with respect to $\mathbf{y}$ but with a negative sign. as $\nabla_{\mu}e^{i\mathbf{k}^T(\mu - \mathbf{y})} = - \nabla_{\mathbf{y}}e^{i\mathbf{k}^T(\mu - \mathbf{y})}$. Then, using integration by parts with respect to $\mathbf{y}$, the following equations can be obtained -</p><p>$$ \begin{align} \nabla_\mu \mathbb{E}_q[f(\mathbf{x})] &amp;= \mathbb{E}_q[\nabla_\mathbf{x}f(\mathbf{x})]\\ \nabla_{\Sigma} \mathbb{E}_q[f(\mathbf{x})] &amp;= \frac{1}{2}\mathbb{E}_q [\nabla_\mathbf{x}^2f(\mathbf{x})] \end{align} $$</p><p>The second formula can be derived though similar arguments.</p><p><strong>Note:</strong> The above two formulae are called as <em>Bonnet's Theorem</em> and <em>Price's Theorem</em> respectively and they can also be derived without using the Characteristic functions or Fourier Transforms. For a gist of the derivation, Refer to Danilo Rezende et.al's paper<sup id=fnref:2><a class=fnref href=#fndef:2>[1]</a></sup>.</p><p>  <em>So, exactly what is that which makes the above formula a trick?</em> <strong>Monte-Carlo methods</strong>. Recall that Monte-Carlo methods essentially compute the expectation through sampling. However, sampling is a discrete process and hence, gradients cannot be computed. This problem can be circumvented by interchanging the gradient and the expectation such that sampling can be done over the gradient. The famous Gaussian re-parameterization trick (using in VAEs) is one solution to overcome the problem. The above trick is a more general solution to compute the gradients for <em>any</em> function expressed as an expectation with respect to a Gaussian.</p><hr><p><table class=fndef id=fndef:2><tr><td class=fndef-backref><a href=#fnref:2>[1]</a></td><td class=fndef-content>Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. <em>Stochastic backpropagation and approximate inference in deep generative models.</em> arXiv preprint arXiv:1401.4082 (2014). <a href=https://arxiv.org/abs/1401.4082>Link</a>. <strong>Additional Reference:</strong> Opper, Manfred, and Cédric Archambeau. <em>The variational Gaussian approximation revisited.</em> Neural computation 21.3 (2009): 786-792 <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b3d7d50f527ab3e7bb31aa8bdd5d110d23ab1ca4">Link</a>.</td></tr></table></p></main><div style="font-size:1.2rem; font-family: 'Overpass'; color: var(--c-4); text-align: center; margin-top: 6em; border-width: 75%; border-top: 1px solid var(--c-4); padding-top: 2em; margin-bottom: 4em;"> &copy; 2025 Anand K Subramanian <span class=vl></span><a href=/license>License</a> <span class=vl></span><a href=/design>Design</a> <span class=vl></span> Built with Kutti <span style="color: #e25555; font-size: 24px;">&hearts;</span></div></body></html>