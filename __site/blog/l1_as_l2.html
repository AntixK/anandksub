<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        throwOnError: false,
        // output: "html",
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true }, // block
          { left: '$', right: '$', display: false }, // Inline
          { left: "\begin{align}", right: "\end{align}", display: true }, // block
          { left: "\begin{aligned}", right: "\end{aligned}", display: true }, // block
        ],
        ignoredTags: [
          "pre",
          "code",
          "script",
          "style",
        ],
        macros: {
          "\\A": "\\mathbb{A}",
          "\\B": "\\mathbb{B}",
          "\\C": "\\mathbb{C}",
          "\\D": "\\mathbb{D}",
          "\\E": "\\mathbb{E}",
          "\\F": "\\mathbb{F}",
          "\\G": "\\mathbb{G}",
          "\\H": "\\mathbb{H}",
          "\\I": "\\mathbb{I}",
          "\\J": "\\mathbb{J}",
          "\\K": "\\mathbb{K}",
          "\\L": "\\mathbb{L}",
          "\\M": "\\mathbb{M}",
          "\\N": "\\mathbb{N}",
          "\\O": "\\mathbb{O}",
          "\\P": "\\mathbb{P}",
          "\\Q": "\\mathbb{Q}",
          "\\R": "\\mathbb{R}",
          "\\S": "\\mathbb{S}",
          "\\T": "\\mathbb{T}",
          "\\U": "\\mathbb{U}",
          "\\V": "\\mathbb{V}",
          "\\W": "\\mathbb{W}",
          "\\X": "\\mathbb{X}",
          "\\Y": "\\mathbb{Y}",
          "\\Z": "\\mathbb{Z}",

          "\\fA": "\\mathcal{A}",
          "\\fB": "\\mathcal{B}",
          "\\fC": "\\mathcal{C}",
          "\\fD": "\\mathcal{D}",
          "\\fE": "\\mathcal{E}",
          "\\fF": "\\mathcal{F}",
          "\\fG": "\\mathcal{G}",
          "\\fH": "\\mathcal{H}",
          "\\fI": "\\mathcal{I}",
          "\\fJ": "\\mathcal{J}",
          "\\fK": "\\mathcal{K}",
          "\\fL": "\\mathcal{L}",
          "\\fM": "\\mathcal{M}",
          "\\fN": "\\mathcal{N}",
          "\\fO": "\\mathcal{O}",
          "\\fP": "\\mathcal{P}",
          "\\fQ": "\\mathcal{Q}",
          "\\fR": "\\mathcal{R}",
          "\\fS": "\\mathcal{S}",
          "\\fT": "\\mathcal{T}",
          "\\fU": "\\mathcal{U}",
          "\\fV": "\\mathcal{V}",
          "\\fW": "\\mathcal{W}",
          "\\fX": "\\mathcal{X}",
          "\\fY": "\\mathcal{Y}",
          "\\fZ": "\\mathcal{Z}",

          "\\x": "\\times",
          "\\vA": "\\mathbf{A}",
          "\\vB": "\\mathbf{B}",
          "\\vC": "\\mathbf{C}",
          "\\vD": "\\mathbf{D}",
          "\\vE": "\\mathbf{E}",
          "\\vF": "\\mathbf{F}",
          "\\vG": "\\mathbf{G}",
          "\\vH": "\\mathbf{H}",
          "\\vI": "\\mathbf{I}",
          "\\vJ": "\\mathbf{J}",
          "\\vK": "\\mathbf{K}",
          "\\vL": "\\mathbf{L}",
          "\\vM": "\\mathbf{M}",
          "\\vN": "\\mathbf{N}",
          "\\vO": "\\mathbf{O}",
          "\\vP": "\\mathbf{P}",
          "\\vQ": "\\mathbf{Q}",
          "\\vR": "\\mathbf{R}",
          "\\vS": "\\mathbf{S}",
          "\\vT": "\\mathbf{T}",
          "\\vU": "\\mathbf{U}",
          "\\vV": "\\mathbf{V}",
          "\\vW": "\\mathbf{W}",
          "\\vX": "\\mathbf{X}",
          "\\vY": "\\mathbf{Y}",
          "\\vZ": "\\mathbf{Z}",

          "\\va": "\\mathbf{a}",
          "\\vb": "\\mathbf{b}",
          "\\vc": "\\mathbf{c}",
          "\\vd": "\\mathbf{d}",
          "\\ve": "\\mathbf{e}",
          "\\vf": "\\mathbf{f}",
          "\\vg": "\\mathbf{g}",
          "\\vh": "\\mathbf{h}",
          "\\vi": "\\mathbf{i}",
          "\\vj": "\\mathbf{j}",
          "\\vk": "\\mathbf{k}",
          "\\vl": "\\mathbf{l}",
          "\\vl": "\\mathbf{l}",
          "\\vm": "\\mathbf{m}",
          "\\vn": "\\mathbf{n}",
          "\\vo": "\\mathbf{o}",
          "\\vp": "\\mathbf{p}",
          "\\vq": "\\mathbf{q}",
          "\\vr": "\\mathbf{r}",
          "\\vs": "\\mathbf{s}",
          "\\vt": "\\mathbf{t}",
          "\\vu": "\\mathbf{u}",
          "\\vv": "\\mathbf{v}",
          "\\vw": "\\mathbf{w}",
          "\\vx": "\\mathbf{x}",
          "\\vy": "\\mathbf{y}",
          "\\vz": "\\mathbf{z}",

          "\\vmu": "\\boldsymbol \\mu",
          "\\vnu": "\\boldsymbol \\nu",
          "\\veps": "\\boldsymbol \\epsilon",
          "\\vtheta": "\\boldsymbol \\theta",
          "\\vomega": "\\boldsymbol \\omega",
          "\\vlambda": "\\boldsymbol \\lambda",
          "\\vzero": "\\mathbf{0}",
          "\\vone": "\\mathbf{1}",
          "\argmin": "\mathop{\mathrm{argmin}} \;",
          "\argmax": "\mathop{\mathrm{argmax}} \;",
          "\\KL": "\\mathop{D_{\\mathrm{KL}}} \\;",

        },
        globalGroup: true,
      });
    });
  </script><link rel=stylesheet href=/_libs/katex/katex.min.css><script defer src=/_libs/katex/katex.min.js crossorigin=anonymous></script><script defer src=/_libs/katex/auto-render.min.js crossorigin=anonymous onload=renderMathInElement(document.body);></script><link rel=stylesheet href=/_css/ak.css><title>L1 Regularization in terms of L2</title></head><body><header><h1><a href=/ >Anand K Subramanian</a><br><br></h1></header><nav class=navbar><span><a href=/blog/ >Blog</a><span class=vl></span></span><span><a href=/notes/ >Notes</a> <span class=vl></span></span><span><a href=/art/ >Art</a> <span class=vl></span></span><span><a href=/CV>CV</a> <span class=vl></span></span><span><a href=/tags>Tags</a></nav><p class=tags><img class=icon-image src=/assets/icons/bookmark.svg alt=clock-icon>&ensp;<span class=pound>#</span>math <span class=pound>#</span>ml </p><p class=tags><img class=icon-image src=/assets/icons/calendar.svg alt=clock-icon>&ensp;18 March 2024 </p><p class=tags><img class=icon-image src=/assets/icons/timer.svg alt=clock-icon>&ensp;3 mins </p><h1>L1 Regularization in terms of L2</h1><p class=tldr>A short note on over-parameterizing the L1 regularizer to make it differentiable</p><main><p>  The $l_1$ regularized optimization problems are quite common in machine learning. They lead to a sparse solution to the modelling problem. Consider the following optimization problem with $l_1$ penalty<sup id=fnref:lnot><a class=fnref href=#fndef:lnot>[1]</a></sup>.</p><p>$$ \min_{\theta \in \R} f(\theta) := h(\theta) + \lambda |\theta| $$<a id=eq:l1 class=anchor></a></p><p>Where $\theta$ are the parameters of the model $f$ to be optimized. Let us study the reparameterization $\theta = \mu \circ \nu$ for all $\theta \in \R$, where $\circ$ represents the element-wise (Hadamard) product of the vectors $\mu$ and $\nu$. This is called the <em>Hadamard product parameterization</em> (HPP) or simply the Hadamard paramterization<sup id=fnref:hoff><a class=fnref href=#fndef:hoff>[2]</a></sup>. Then the equation <span class=eqref>(<a href=#eq:l1>1</a>)</span> can be written as</p><p>$$ \begin{aligned} \min_{\theta \in \R} f(\theta) &amp;= h(\theta) + \lambda |\theta| \\ % &amp;\leq h(\theta) + \lambda \|\theta\|_2 \\ &amp;=h(\mu \circ \nu) + \lambda \sum_i |\mu_i \nu_i| \\ &amp;= h(\mu \circ \nu) + \lambda \sum_i \sqrt{\mu_i^2 \nu_i^2} \\ &amp;\leq h(\mu \circ \nu) + \lambda \bigg ( \sum_i \frac{\mu_i^2 + \nu_i^2}{2} \bigg ) &amp;&amp; {\color{OrangeRed}\text{AM-GM Inequality}}\\ &amp;\leq h(\mu \circ \nu) + \frac{\lambda}{2} \big ( \| \mu\|_2^2 + \|\nu\|_2^2 \big ) \\ &amp; \leq \min_{\mu, \nu \in \R} g(\mu, \nu) \end{aligned} $$<a id=eq:had_sub class=anchor></a></p><p>Therefore, the auxiliary optimization function is $$ \min_{\mu, \nu \in \R} g(\mu, \nu) := h(\mu \circ \nu) + \frac{\lambda}{2} \big ( \| \mu\|_2^2 + \|\nu\|_2^2 \big ) $$<a id=eq:had_reparam class=anchor></a></p><p>The above function is constructed rather purposefully to satisfy the following properties.</p><ol><li>The over-parameterized function $g(\mu, \nu)$ is an upper bound to our optimization function $f(\theta)$, <em>i.e.</em> $g(\mu,\nu) \geq f(\mu \circ \nu), \forall \mu,\nu \in \R$. The equality occurs where $|\mu| = |\nu|$ as can be observed from equation <span class=eqref>(<a href=#eq:had_sub>2</a>)</span>.</li><li>The function $g(\mu, \nu)$ satisfies the equality $\inf f = \inf g$. This means that the optima (minima in this case) of $f$ is equal to the optima of $g$. In other words, if $\mu$ and $\nu$ are optimal (in terms of $l_2$), then $\mu \circ \nu$ is an optimal (in terms of $l_1$) value of $\theta$.</li></ol><p>Moreover, since $g(\mu, \nu)$ uses the $l_2$ regularization, it is differentiable and biconvex. Thus, the auxiliary function is amenable to gradient-based optimization algorithms like SGD, Adam, etc. Interestingly, Ziyin et. al.<sup id=fnref:spred><a class=fnref href=#fndef:spred>[3]</a></sup> also proved a stronger property compared to $\inf f = \inf g$. They show that at all stationary points of <span class=eqref>(<a href=#eq:had_reparam>3</a>)</span>, $|\mu_i| = |\nu_i| \; \forall i $ and every local minima of $g(\mu, \nu)$, given by equation <span class=eqref>(<a href=#eq:had_reparam>3</a>)</span> is a local minima of $f(x)$, given by equation <span class=eqref>(<a href=#eq:l1>1</a>)</span>.</p><img style=width:80%;min-width:300px; src=/media/post_images/HP_opt.webp alt=Hadamard_optimization><p class=caption-text>Optimization trajectory of l1 and HP regularizations</p><p>The above figure shows that, for the same initial conditions and optimization parameters, the $l_1$ regularized objective function (Griewank, in this case) gets stuck in a local minima, while the Hadamard-parameterized function correctly reaches the global minima, which is at $(0, 0)$. Note that the $l_1$ regularized objective can be used with Pytorch's SGD optimizer, as they use a <em>subgradient</em> of 1 at the non-differentiable point. But this is a convention, as the subgradient of $|x|$ at $0$ is the set $[-1, 1]$.</p><p>  Lastly, similar to interpreting the $l_1$ and $l_2$ regularizers in least-squares problems as Laplacian and Gaussian priors respectively, the equation <span class=eqref>(<a href=#eq:had_reparam>3</a>)</span> can also be examined through a probabilistic framework. Here, with the parameters $\mu$ and $\nu$ subject to $l_2$ norm regularization, they can be construed as being governed by a Gaussian prior distribution $\mathcal{N}(0, 2/\lambda)$. This implies a regularization effect on the components $\mu$ and $\nu$.</p><hr><p><table class=fndef id=fndef:lnot><tr><td class=fndef-backref><a href=#fnref:lnot>[1]</a></td><td class=fndef-content>TIL, that the notation $l_p$ is reserved for vectors while the uppercase notation $L_p$ is for operators and functions. So, the appropriate notations are $l_2(x)$ and $L_2[\phi(x)]$ for vector $x$ and the function $\phi(x)$ respectively.</td></tr></table></p><p><table class=fndef id=fndef:hoff><tr><td class=fndef-backref><a href=#fnref:hoff>[2]</a></td><td class=fndef-content>Hoff, P. D. (2017). <em>Lasso, fractional norm and structured sparse estimation using a hadamard product parametrization</em>. Comput. Stat. Data An., 115:186–198. <a href=https://arxiv.org/abs/1611.00040>ArXiv Link</a>.</td></tr></table></p><p><table class=fndef id=fndef:spred><tr><td class=fndef-backref><a href=#fnref:spred>[3]</a></td><td class=fndef-content>Ziyin, L. &amp; Wang, Z. (2023). spred: Solving L1 Penalty with SGD. <em>Proceedings of the 40th International Conference on Machine Learning</em>, in <em>Proceedings of Machine Learning Research</em> 202:43407-43422 <a href=https://arxiv.org/abs/2210.01212>ArXiv Link</a&lt;></tr></table></p></main><div style="font-size:1.2rem; font-family: 'Overpass'; color: var(--c-4); text-align: center; margin-top: 6em; border-width: 75%; border-top: 1px solid var(--c-4); padding-top: 2em; margin-bottom: 4em;"> &copy; 2024 Anand K Subramanian <span class=vl></span><a href=/license>License</a> <span class=vl></span><a href=/design>Design</a> <span class=vl></span> Built with Kutti <span style="color: #e25555; font-size: 24px;">&hearts;</span></div></body></html>