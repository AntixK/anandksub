<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        throwOnError: false,
        // output: "html",
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true }, // block
          { left: '$', right: '$', display: false }, // Inline
          { left: "\begin{align}", right: "\end{align}", display: true }, // block
          { left: "\begin{aligned}", right: "\end{aligned}", display: true }, // block
        ],
        ignoredTags: [
          "pre",
          "code",
          "script",
          "style",
        ],
        macros: {
          "\\A": "\\mathbb{A}",
          "\\B": "\\mathbb{B}",
          "\\C": "\\mathbb{C}",
          "\\D": "\\mathbb{D}",
          "\\E": "\\mathbb{E}",
          "\\F": "\\mathbb{F}",
          "\\G": "\\mathbb{G}",
          "\\H": "\\mathbb{H}",
          "\\I": "\\mathbb{I}",
          "\\J": "\\mathbb{J}",
          "\\K": "\\mathbb{K}",
          "\\L": "\\mathbb{L}",
          "\\M": "\\mathbb{M}",
          "\\N": "\\mathbb{N}",
          "\\O": "\\mathbb{O}",
          "\\P": "\\mathbb{P}",
          "\\Q": "\\mathbb{Q}",
          "\\R": "\\mathbb{R}",
          "\\S": "\\mathbb{S}",
          "\\T": "\\mathbb{T}",
          "\\U": "\\mathbb{U}",
          "\\V": "\\mathbb{V}",
          "\\W": "\\mathbb{W}",
          "\\X": "\\mathbb{X}",
          "\\Y": "\\mathbb{Y}",
          "\\Z": "\\mathbb{Z}",

          "\\fA": "\\mathcal{A}",
          "\\fB": "\\mathcal{B}",
          "\\fC": "\\mathcal{C}",
          "\\fD": "\\mathcal{D}",
          "\\fE": "\\mathcal{E}",
          "\\fF": "\\mathcal{F}",
          "\\fG": "\\mathcal{G}",
          "\\fH": "\\mathcal{H}",
          "\\fI": "\\mathcal{I}",
          "\\fJ": "\\mathcal{J}",
          "\\fK": "\\mathcal{K}",
          "\\fL": "\\mathcal{L}",
          "\\fM": "\\mathcal{M}",
          "\\fN": "\\mathcal{N}",
          "\\fO": "\\mathcal{O}",
          "\\fP": "\\mathcal{P}",
          "\\fQ": "\\mathcal{Q}",
          "\\fR": "\\mathcal{R}",
          "\\fS": "\\mathcal{S}",
          "\\fT": "\\mathcal{T}",
          "\\fU": "\\mathcal{U}",
          "\\fV": "\\mathcal{V}",
          "\\fW": "\\mathcal{W}",
          "\\fX": "\\mathcal{X}",
          "\\fY": "\\mathcal{Y}",
          "\\fZ": "\\mathcal{Z}",

          "\\x": "\\times",
          "\\vA": "\\mathbf{A}",
          "\\vB": "\\mathbf{B}",
          "\\vC": "\\mathbf{C}",
          "\\vD": "\\mathbf{D}",
          "\\vE": "\\mathbf{E}",
          "\\vF": "\\mathbf{F}",
          "\\vG": "\\mathbf{G}",
          "\\vH": "\\mathbf{H}",
          "\\vI": "\\mathbf{I}",
          "\\vJ": "\\mathbf{J}",
          "\\vK": "\\mathbf{K}",
          "\\vL": "\\mathbf{L}",
          "\\vM": "\\mathbf{M}",
          "\\vN": "\\mathbf{N}",
          "\\vO": "\\mathbf{O}",
          "\\vP": "\\mathbf{P}",
          "\\vQ": "\\mathbf{Q}",
          "\\vR": "\\mathbf{R}",
          "\\vS": "\\mathbf{S}",
          "\\vT": "\\mathbf{T}",
          "\\vU": "\\mathbf{U}",
          "\\vV": "\\mathbf{V}",
          "\\vW": "\\mathbf{W}",
          "\\vX": "\\mathbf{X}",
          "\\vY": "\\mathbf{Y}",
          "\\vZ": "\\mathbf{Z}",

          "\\va": "\\mathbf{a}",
          "\\vb": "\\mathbf{b}",
          "\\vc": "\\mathbf{c}",
          "\\vd": "\\mathbf{d}",
          "\\ve": "\\mathbf{e}",
          "\\vf": "\\mathbf{f}",
          "\\vg": "\\mathbf{g}",
          "\\vh": "\\mathbf{h}",
          "\\vi": "\\mathbf{i}",
          "\\vj": "\\mathbf{j}",
          "\\vk": "\\mathbf{k}",
          "\\vl": "\\mathbf{l}",
          "\\vl": "\\mathbf{l}",
          "\\vm": "\\mathbf{m}",
          "\\vn": "\\mathbf{n}",
          "\\vo": "\\mathbf{o}",
          "\\vp": "\\mathbf{p}",
          "\\vq": "\\mathbf{q}",
          "\\vr": "\\mathbf{r}",
          "\\vs": "\\mathbf{s}",
          "\\vt": "\\mathbf{t}",
          "\\vu": "\\mathbf{u}",
          "\\vv": "\\mathbf{v}",
          "\\vw": "\\mathbf{w}",
          "\\vx": "\\mathbf{x}",
          "\\vy": "\\mathbf{y}",
          "\\vz": "\\mathbf{z}",

          "\\valpha": "\\boldsymbol \\alpha",
          "\\vbeta": "\\boldsymbol \\beta",
          "\\vgamma": "\\boldsymbol \\gamma",
          "\\vdelta": "\\boldsymbol \\delta",
          "\\vepsilon": "\\boldsymbol \\epsilon",
          "\\vzeta": "\\boldsymbol \\zeta",
          "\\veta": "\\boldsymbol \\eta",
          "\\vtheta": "\\boldsymbol \\theta",
          "\\viota": "\\boldsymbol \\iota",
          "\\vkappa": "\\boldsymbol \\kappa",
          "\\vlambda": "\\boldsymbol \\lambda",
          "\\vmu": "\\boldsymbol \\mu",
          "\\vnu": "\\boldsymbol \\nu",
          "\\vxi": "\\boldsymbol \\xi",
          "\\vomicron": "\\boldsymbol \\omicron",
          "\\vpi": "\\boldsymbol \\pi",
          "\\vrho": "\\boldsymbol \\rho",
          "\\vsigma": "\\boldsymbol \\sigma",
          "\\vtau": "\\boldsymbol \\tau",
          "\\vupsilon": "\\boldsymbol \\upsilon",
          "\\vphi": "\\boldsymbol \\phi",
          "\\vchi": "\\boldsymbol \\chi",
          "\\vpsi": "\\boldsymbol \\psi",
          "\\vomega": "\\boldsymbol \\omega",

          "\\vzero": "\\mathbf{0}",
          "\\vone": "\\mathbf{1}",
          "\argmin": "\mathop{\mathrm{argmin}} \;",
          "\argmax": "\mathop{\mathrm{argmax}} \;",
          "\\KL": "\\mathop{D_{\\mathrm{KL}}} \\;",

        },
        globalGroup: true,
      });
    });
  </script><link rel=stylesheet href=/_libs/katex/katex.min.css><script defer src=/_libs/algorithm/algotype.js crossorigin=anonymous></script><script defer src=/_libs/katex/katex.min.js crossorigin=anonymous>
  </script></script><script defer src=/_libs/katex/auto-render.min.js crossorigin=anonymous onload=renderMathInElement(document.body);></script><link rel=stylesheet href=/_css/ak.css><link rel=icon href=/assets/icons/favicon.ico type=image/x-icon><title>VAEs, Diffusion, and Mutual Information</title></head><body><header><h1><a href=/ >Anand K Subramanian</a><br><br></h1></header><nav class=navbar><span><a href=/blog/ >Blog</a><span class=vl></span></span><span><a href=/notes/ >Notes</a> <span class=vl></span></span><span><a href=/art/ >Art</a> <span class=vl></span></span><span><a href=/CV>CV</a> <span class=vl></span></span><span><a href=/tags>Tags</a></nav><p class=tags><img class=icon-image src=/assets/icons/bookmark.svg alt=clock-icon>&ensp;<span class=pound>#</span>math <span class=pound>#</span>ml <span class=pound>#</span>code <span class=pound>#</span>information-theory </p><p class=tags><img class=icon-image src=/assets/icons/calendar.svg alt=clock-icon>&ensp;2 October 2024 </p><p class=tags><img class=icon-image src=/assets/icons/timer.svg alt=clock-icon>&ensp;9 mins </p><h1>VAEs, Diffusion, and Mutual Information</h1><p class=tldr>Why did diffusion win over VAEs.</p><main><p>  Consider the problem setup where we have a dataset $\vx \in \fX$ and and a latent variable $\vz \in \fZ$ and we want to learn the original data distribution $p_{\fX}(\vx)$ using the latent variable $\vz$. We assume a model distribution $p(\vx)$ such that</p><p>$$ p(\vx) = \int p(\vx|\vz)p(\vz)d\vz $$<a id=eq:ldm class=anchor></a></p><p>Where $p(\vx|\vz)$ is the likelihood of the data given the latent variable and $p(\vz)$ is the prior distribution of the latent variable. This is a general setup that allows for a flexible modelling of the data distribution with the ability to generate new samples. For instance, if $p(\vz)$ is a categorical distribution and $p(\vx | \vz)$ is a Gaussian, then the equation <span class=eqref>(<a href=#eq:ldm>1</a>)</span> represents a Gaussian Mixture Model. The support of $p(\vz)$ determines whether it is a discrete latent model or a continuous latent model. In fact, the choice of $p(\vz)$ can be quite complex and there has been a lot of such proposals in the literature.</p><p>Whatever be the choice of $p(\vx|\vz)$ and $p(\vz)$, the goal is to match the model distribution $p(\vx)$ and the actual data distribution $p_{\fX}(\vx)$ as closely as possible. One common distance-like measure between two distributions is the Kullback-Leibler (KL) divergence. The KL divergence between $p(\vx)$ and $p_{\fX}(\vx)$ is given by</p><p>$$ \begin{aligned} \KL \left (p_{\fX}(\vx) || p(\vx) \right ) &amp;= \int p_{\fX}(\vx) \log \frac{p_{\fX}(\vx)}{p(\vx)}d\vx \\ &amp;= \int p_{\fX}(\vx) \log p_{\fX}(\vx)d\vx - \int p_{\fX}(\vx) \log p(\vx)d\vx \\ &amp;= -\H\left [p_{\fX}(\vx) \right ] - \E_{p_{\fX}(\vx)}[\log p(\vx)] \end{aligned} $$ <a id=eq:kl_div class=anchor></a></p><p>Where $\H$ is the differential entropy of the data distribution. So, we can only control the second term in the above equation. In other words, minimizing the above KL divergence is equivalent to maximizing the likelihood of the given data samples under the model distribution $p(\vx)$. Therefore, every latent variable model inevitably involves maximizing the likelihood of the data samples under the model distribution.</p><h2>Two-Way Modelling in VAEs</h2><p>  Observe that the likelihood $p(\vx|\vz)$ maps the latent space $\fZ$ to the data space $\fX$. The learning objective given in equation <span class=eqref>(<a href=#eq:kl_div>2</a>)</span> is independent of the inverse mapping $p(\vz|\vx)$ from the data space to the latent space. Indeed, there have been approaches where only the likelihood (or the forward mapping from $\fZ \to \fX$) is optimized - GANs, for example.</p><p>Variational Autoencoders (VAEs), on the other hand, model both the forward and reverse mappings. Note that the reverse mapping $p(\vz|\vx)$ is the posterior distribution of the latent variable model. In other words, to model one, we need to model the other - a chicken and egg problem. Other techniques like the Expectation-Maximization (EM) algorithm alternates between computing the posterior (E-step) and using that to maximize the likelihood (M-step).</p><p>VAEs, quite beautifully, solve this problem by incorporating the posterior distribution in the above learning objective and in a most general way - using Variational Inference (VI). VAEs define a likelihood distribution (forward mapping) and a prior as these are straight-forward to model. The posterior distribution (reverse mapping) is approximated by a variational distribution $q(\vz|\vx)$. Now, second term in the equation <span class=eqref>(<a href=#eq:kl_div>2</a>)</span> can be written as</p><p>$$ \begin{aligned} \E_{p_{\fX}(\vx)}\left [\log p(\vx) \right ] &amp;= \E_{p_{\fX}(\vx)} \left [\log \int p(\vx|\vz)p(\vz)d\vz \right ] \\ &amp;= \E_{p_{\fX}(\vx)}\left [\log \int q(\vz|\vx)\frac{p(\vx|\vz)p(\vz)}{q(\vz|\vx)}d\vz \right ] \\ &amp;= \E_{p_{\fX}(\vx)} \log \left [ \E_{q(\vz|\vx)} \left [ \frac{p(\vx|\vz)p(\vz)}{q(\vz|\vx)} \right ] \right ]\\ &amp;\geq \E_{p_{\fX}(\vx)}\E_{q(\vz|\vx)} \left [\log \frac{p(\vx|\vz)p(\vz)}{q(\vz|\vx)} \right ] \qquad {\color{OrangeRed} \text{(Jensen's Inequality)}} \\ \end{aligned} $$</p><p>The above VI objective is generic enough to be applicable for any latent variable model - given the likelihood, prior, and the variational distributions. This objective is called the Evidence Lower Bound (ELBO). In practice, VAEs optimize the ELBO written as</p><p>$$ \begin{aligned} \E_{p_{\fX}(\vx)}\E_{q(\vz|\vx)} \left [\log \frac{p(\vx|\vz)p(\vz)}{q(\vz|\vx)} \right ] &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] - \E_{q(\vz|\vx)} \log q(\vz|\vx) + \E_{q(\vz|\vx)} \log p(\vz) d\vz \\ &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] - \int q(\vz|\vx) \log \frac{q(\vz|\vx)}{p(\vz)} d\vz \\ &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] - \KL \left (q(\vz|\vx) || p(\vz) \right ) \\ \end{aligned} $$<a id=eq:elbo_vae class=anchor></a></p><h3>ELBO and Mutual Information</h3><p>$$ \begin{aligned} \E_{p_{\fX}(\vx)}\left [\log p(\vx) \right ] &amp;\geq \E_{p_{\fX}(\vx)}\E_{q(\vz|\vx)} \left [\log \frac{p(\vx|\vz)p(\vz)}{q(\vz|\vx)} \right ] \\ &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} \left [\log p(\vx|\vz) + \log p(\vz) - \log q(\vz|\vx) \right ] \\ &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] + \E_{ \vz \sim q(\vz|\vx)} [\log p(\vz)] - \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log q(\vz|\vx) ] \\ &amp;=\E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] + \E_{ \vz \sim q(\vz|\vx)} [\log p(\vz)] - \int q(\vz|\vx) p_{\fX}(\vx) \log q(\vz|\vx) d\vx d\vz \\ &amp;=\E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] + \E_{ \vz \sim q(\vz|\vx)} [\log p(\vz)] + \H [\vz|\vx]\\ &amp;=\E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] + \E_{ \vz \sim q(\vz|\vx)} [\log p(\vz)] + \H [\vz] - \I[\vx ; \vz]\\ \end{aligned} $$ <a id=eq:elbo_mi class=anchor></a></p><p>From the above equation, note that the first term is the qualitry of mapping from $\fX \to \fZ \to \fX$. However, the last term specifies that the mutual information (non-negative) between $\vx$ and $\vz$ must be reduced. This constrasts with the first term where a robust mapping between $\vx$ and $\vz$ is necessary for good reconstruction.</p><p>Alternatively, note that the variational posterior $q(\vz|\vx)$ maps each data sample to a corresponding latent variable $\vz$ (amortized VI). With this, we can view the KL divergence in equation <span class=eqref>(<a href=#eq:elbo_vae>4</a>)</span> as a regularizer of sorts - one that minimizes the overlap between the variational posterior and the prior. One way to minimize this overlap is to reduce the mutual information between the data and the latent variables.</p><img style=width:85%;min-width:400px; src=/media/post_images/vae_collapse.svg alt="VAE Training Problems"><p class="caption-text ">Illustration of some training issues with VAEs - especially with the mapping from data (black dots) to the prior distribution (black ellipses) by the amortized variational posterior (purple circles). Either the posterior can overlap (right) or can be disentangled but without learning any stochastic dependencies (left). Both results in a low mutual information between data and the latent variables.</p><p>TODO: Write more about posterior collapse.</p><p>Clearly, the ELBO contains two contrasting terms that work against each other. One often comes across the term <em>disentanglement</em> in VAE literature. This simply means that the latent variables are close to being independent of each other and that each latent variable generates a specific data sample. In simpler terms, the latent space, idealy, should be a compressed representation of the data.</p><p>There are many approaches that address this issue<sup id=fnref:vaeprior><a class=fnref href=#fndef:vaeprior>[1]</a></sup>, and one very popular and simple approach is to bias the KL term in equation <span class=eqref>(<a href=#eq:elbo_vae>4</a>)</span>, and hence the mutual information, to limit the posterior collapse. This is called the $\beta$-VAE. $$ \begin{aligned} \beta\mathrm{-ELBO} &amp;= \E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] - \beta \KL \left (q(\vz|\vx) || p(\vz) \right ) \\ &amp;=\E_{\vx, \vz \sim p_{\fX}(\vx)q(\vz|\vx)} [\log p(\vx|\vz)] + \beta \E_{ \vz \sim q(\vz|\vx)} [\log p(\vz)] + \beta\H [\vz] - \beta\I[\vx ; \vz]\\ \end{aligned} $$<a id=eq:beta_vae class=anchor></a></p><h3>Hierarchical VAEs</h3><p>Before we delve into diffusion models, there is another extension of VAEs called Hierarchical VAEs (HVAEs). Instead of having a single latent distribution, HVAEs have a hierarchy of latent distributions. The idea is that the prior distribution can be more complex, as the prior at each level is</p><img style=width:50%;min-width:300px; src=/media/post_images/lvae.svg alt="VAE with Hierarchical Priors"><p class=caption-text>VAE with Hierarchical Priors</p><h2>Diffusion Models</h2><p>So, how do diffusion models address this problem? Remember when we mentioned that the choice of $p(\vz)$ can be quite complex? If HAVEs extend the prior distribution hierarchicaly, diffusion models extend the priors sequentially (or temporally).</p><p>if you think about it, the latent space is no different from that of the data space. Both are metric spaces. We would like to assume that the latent space is a compressed representation of the data space. What if we can move away from this idea and think of the latent space as simple the data space but just at a different time step? The data space itselv evolves over time.</p><p>The ELBO for diffusion models can be written as</p><p>$$ \begin{aligned} \E_{p_{\fX}(\vx)}\left [\log p(\vx) \right ] &amp;\geq \E_{p_{\fX}(\vx)}\E_{q(\vx_{1:T}|\vx_0)}\left [ \log \frac{p(\vx_{0:T})}{q(\vx_{1:T}|\vx_0)} \right ]\\ &amp;=\E_{p_{\fX}(\vx)}\E_{q(\vx_{1:T}|\vx_0)} \left [ \log \frac{p(\vx_T)p(\vx_0|\vx_1)}{q(\vx_T|\vx_{T-1})} \right ] + \E_{p_{\fX}(\vx)}\E_{q(\vx_{1:T}|\vx_0)} \left [ \log \prod_{t=1}^{T-1} \frac{p(\vx_t|\vx_{t+1})}{q(\vx_t|\vx_{t-1})} \right ] \end{aligned} $$</p><br><div class=important><p>How are diffusion models different from flow-based priors?</p></div><p>Thesis: VAE's ELBO minimizes the MI. There has been a lot of effort in addressing this problem. How about diffusion models? Diffusion models are very similar to VAEs, and yet they produce images with very high fidelity and less prone to posterior collapse.</p><p>Write about Variational lower bound on MI. (https://papers.nips.cc/paper_files/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf0). InfoMax principle.</p><p>https://openreview.net/pdf?id=rkxoh24FPH</p><p>Explain VAE's ELBO and Mutual Information (https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html)</p><p>Representations, Disentanglement, and mutual information.</p><p>Does betaVAE improve? (https://arxiv.org/pdf/1612.00410)</p><p>How about Diffusion models?</p><p>Should you maximize MI or Minimize MI? (https://arxiv.org/pdf/2108.12734)</p><p>Diffusion Models and Mutual Information (https://openreview.net/pdf?id=UvmDCdSPDOW)</p><p>https://openreview.net/pdf?id=X6tNkN6ate</p><p>https://sander.ai/2023/07/20/perspectives.html</p><hr><p><table class=fndef id=fndef:vaeprior><tr><td class=fndef-backref><a href=#fnref:vaeprior>[1]</a></td><td class=fndef-content>One way to address this issue is to modify the ELBO objective. The other way is to use a more expressive prior distribution. Even if we let the posterior collapse to the prior, if the prior is expressive enough, we can still generate diverse samples. This <a href=https://jmtomczak.github.io/blog/7/7_priors.html>link</a> provides a great overview of many of the popular prior used in VAE litetature.</td></tr></table></p></main><div width=100% style="font-size:1.2rem; font-family: 'Overpass'; color: var(--c-4); text-align: center; margin-top: 6em; border-width: 75%; border-top: 1px solid var(--c-4); padding-top: 2em; margin-bottom: 4em;"> &copy; 2025 Anand K Subramanian <span class=vl></span><a href=/license>License</a> <span class=vl></span><a href=/design>Design</a> <span class=vl></span> Built with Kutti &nbsp; <img class=icon-image src=/assets/icons/heart.svg></div></body></html>